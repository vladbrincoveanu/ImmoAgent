#!/usr/bin/env python3
"""
Simple derStandard scraper test
Tests basic functionality without complex logic
"""

import sys, os
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'Project'))

import time

# Add Project directory to path for imports
# sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'Project'))

from Project.Application.scraping.derstandard_scraper import DerStandardScraper
from Project.Integration.mongodb_handler import MongoDBHandler
from Project.Application.helpers.utils import load_config

def main():
    """Simple test function"""
    print("ğŸš€ SIMPLE DERSTANDARD SCRAPER TEST")
    print("=" * 50)
    
    # Load config
    config = load_config()
    print(f"âœ… Config loaded: {config.get('mongodb_uri', 'No URI')}")
    
    # Test MongoDB connection
    print("ğŸ”§ Testing MongoDB connection...")
    mongo_handler = MongoDBHandler(uri=config.get('mongodb_uri'))
    test_doc = {'url': 'test-simple', 'source': 'derstandard', 'title': 'Simple Test'}
    insert_result = mongo_handler.insert_listing(test_doc)
    print(f"âœ… MongoDB test insert: {insert_result}")
    
    # Get current count
    current_count = mongo_handler.collection.count_documents({"source": "derstandard"})
    print(f"ğŸ“Š Current derStandard count: {current_count}")
    
    # Initialize scraper
    print("ğŸ”§ Initializing scraper...")
    scraper = DerStandardScraper(use_selenium=True)
    print("âœ… Scraper initialized")
    
    # Test URL extraction
    test_url = "https://immobilien.derstandard.at/suche/wien/kaufen-wohnung?roomCountFrom=3"
    print(f"ğŸ” Testing URL extraction from: {test_url}")
    
    try:
        urls = scraper.extract_listing_urls(test_url, max_pages=1)
        print(f"âœ… Found {len(urls)} URLs")
        
        if urls:
            # Test scraping first URL
            first_url = urls[0]
            print(f"ğŸ” Testing scraping: {first_url}")
            
            # Check if already exists
            if not mongo_handler.listing_exists(first_url):
                listing = scraper.scrape_single_listing(first_url)
                
                if listing:
                    print(f"âœ… Successfully scraped: {listing.title}")
                    
                    if scraper.meets_criteria(listing):
                        print("âœ… Matches criteria")
                        
                        listing_dict = scraper._ensure_serializable(listing)
                        
                        if mongo_handler.insert_listing(listing_dict):
                            print("ğŸ’¾ Successfully saved to MongoDB!")
                            new_count = mongo_handler.collection.count_documents({"source": "derstandard"})
                            print(f"ğŸ“Š New count: {new_count}")
                        else:
                            print("âŒ Failed to save to MongoDB")
                    else:
                        print("âŒ Does not match criteria")
                else:
                    print("âŒ Failed to scrape listing")
            else:
                print("â­ï¸  Already exists in database")
        else:
            print("âŒ No URLs found")
            
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Clean up
        if hasattr(scraper, 'driver') and scraper.driver:
            scraper.driver.quit()
            print("ğŸ§¹ Selenium driver closed")
        
        mongo_handler.close()
        print("ğŸ§¹ MongoDB connection closed")

if __name__ == "__main__":
    main() 